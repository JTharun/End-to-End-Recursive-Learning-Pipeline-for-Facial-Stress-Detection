{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bfcb7-73f6-4d9f-ae19-9531153801fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "count = 5\n",
    "def dreamer_now(stress_value):\n",
    "    if stress_value >= 0.4:\n",
    "        print(\"Here are a few happy songs to help you relax:\")\n",
    "        for i in range(1, 6):\n",
    "            print(happy_songs[i])\n",
    "    elif stress_value >= 0.5:\n",
    "        print(\"Here are a few happy songs to cheer you up:\")\n",
    "        for i in range(16, 21):\n",
    "            print(happy_songs[i])\n",
    "    else:\n",
    "        print(\"Here are a few happy songs to pump you up:\")\n",
    "        for i in range(44, 50):\n",
    "            print(happy_songs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f97ee-b97c-48cc-b2c2-91212bc38f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_songs = {\n",
    "    1: \"Walking on Sunshine - Katrina and the Waves\",\n",
    "    2: \"Don't Stop Believin' - Journey\",\n",
    "    3: \"I Wanna Dance with Somebody - Whitney Houston\",\n",
    "    4: \"Happy - Pharrell Williams\",\n",
    "    5: \"Uptown Funk - Mark Ronson ft. Bruno Mars\",\n",
    "    6: \"September - Earth, Wind & Fire\",\n",
    "    7: \"I'm a Believer - The Monkees\",\n",
    "    8: \"Celebration - Kool & The Gang\",\n",
    "    9: \"Dancing Queen - ABBA\",\n",
    "    10: \"Hey Ya! - OutKast\",\n",
    "    11: \"Can't Stop the Feeling! - Justin Timberlake\",\n",
    "    12: \"Girls Just Want to Have Fun - Cyndi Lauper\",\n",
    "    13: \"Shake It Off - Taylor Swift\",\n",
    "    14: \"I Will Survive - Gloria Gaynor\",\n",
    "    15: \"Good Vibrations - The Beach Boys\",\n",
    "    16: \"Wake Me Up Before You Go-Go - Wham!\",\n",
    "    17: \"I'm Still Standing - Elton John\",\n",
    "    18: \"All Star - Smash Mouth\",\n",
    "    19: \"You Make My Dreams - Hall & Oates\",\n",
    "    20: \"Sweet Caroline - Neil Diamond\",\n",
    "    21: \"What a Wonderful World - Louis Armstrong\",\n",
    "    22: \"Valerie - Mark Ronson ft. Amy Winehouse\",\n",
    "    23: \"Stayin' Alive - Bee Gees\",\n",
    "    24: \"Bohemian Rhapsody - Queen\",\n",
    "    25: \"I Want You Back - The Jackson 5\",\n",
    "    26: \"I Feel Good - James Brown\",\n",
    "    27: \"Mamma Mia - ABBA\",\n",
    "    28: \"Footloose - Kenny Loggins\",\n",
    "    29: \"Ain't No Mountain High Enough - Marvin Gaye & Tammi Terrell\",\n",
    "    30: \"Get Lucky - Daft Punk ft. Pharrell Williams\",\n",
    "    31: \"You Can't Hurry Love - The Supremes\",\n",
    "    32: \"Sweet Child O' Mine - Guns N' Roses\",\n",
    "    33: \"Kiss - Prince\",\n",
    "    34: \"Dancing in the Street - Martha & The Vandellas\",\n",
    "    35: \"I Want to Hold Your Hand - The Beatles\",\n",
    "    36: \"All You Need Is Love - The Beatles\",\n",
    "    37: \"Twist and Shout - The Beatles\",\n",
    "    38: \"Ain't No Sunshine - Bill Withers\",\n",
    "    39: \"Here Comes the Sun - The Beatles\",\n",
    "    40: \"Three Little Birds - Bob Marley\",\n",
    "    41: \"One Love - Bob Marley\",\n",
    "    42: \"Could You Be Loved - Bob Marley\",\n",
    "    43: \"What's Up? - 4 Non Blondes\",\n",
    "    44: \"Eye of the Tiger - Survivor\",\n",
    "    45: \"Livin' on a Prayer - Bon Jovi\",\n",
    "    46: \"You Give Love a Bad Name - Bon Jovi\",\n",
    "    47: \"It's My Life - Bon Jovi\",\n",
    "    48: \"Don't Stop Me Now - Queen\",\n",
    "    49: \"We Will Rock You - Queen\",\n",
    "    50: \"I Want It That Way - Backstreet Boys\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cda490d-4c5e-46ff-a522-1321f9af3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flow=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d27e1-be99-497c-9e95-228d9384a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(img,label):\n",
    "    df[\"Image\"]=img\n",
    "    df[\"Label\"]=label\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2dc3e-e32e-48f0-ba34-574575cc4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.to_csv(\"Data_flow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d1a4f-3a33-4b88-8fba-37806c0a8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def eye_brow_distance(leye,reye):\n",
    "    #global points\n",
    "    distq = dist.euclidean(leye,reye)\n",
    "    points.append(int(distq))\n",
    "    return distq\n",
    "\n",
    "def normalize_values(points,disp):\n",
    "    normalized_value = abs(disp - np.min(points))/abs(np.max(points) - np.min(points))\n",
    "    stress_value = np.exp(-(normalized_value))\n",
    "    if stress_value>=50:\n",
    "        stress_status = \"Current Stress value\"\n",
    "    else:\n",
    "        stress_status = \"Current stress value\"\n",
    "    return stress_value, stress_status\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "points = []\n",
    "\n",
    "while(True):\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = imutils.resize(frame, width = 500, height = 500)\n",
    "\n",
    "    # preprocessing the image\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for detection in detections:\n",
    "        x, y, w, h = detection\n",
    "\n",
    "        leye = []\n",
    "        reye = []\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 5)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            if ex < w/2:\n",
    "                leye = (int(ex+ew/2+x), int(ey+eh/2+y))\n",
    "                #cv2.circle(frame, leye, 2, (0, 0, 255), 2)\n",
    "            else:\n",
    "                reye = (int(ex+ew/2+x), int(ey+eh/2+y))\n",
    "                #cv2.circle(frame, reye, 2, (0, 0, 255), 2)\n",
    "\n",
    "        if leye and reye:\n",
    "\n",
    "            disp = eye_brow_distance(leye,reye)\n",
    "            stress_value, stress_status = normalize_values(points, disp)\n",
    "            cv2.putText(frame, f\"Stress value: {stress_value :.2f}\", (x, y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            pipeline(frame,stress_value)\n",
    "            \n",
    "            while count < 1:\n",
    "                t1 = threading.Thread(target=dreamer_now, args=(stress_value,))\n",
    "                t1.start()\n",
    "                count += 1\n",
    "                t1.join()\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4269e51-9125-480b-a54f-f834e14eca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296c455-4ee0-4954-9f9e-7364f452dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07d134-f920-465d-be23-4c9d3f7eadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2851ed48-0b01-4c58-bbe9-b72ee1339b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to start recording...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "ename": "ParameterError",
     "evalue": "Input signal must be provided to compute a spectrogram",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6812\\1541425913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Convert the audio to a spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mspectrogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_spectrogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m22050\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Predict the emotion from the spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6812\\1541425913.py\u001b[0m in \u001b[0;36mextract_spectrogram\u001b[1;34m(audio, sr, n_fft, hop_length)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \"\"\"\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Compute the Mel spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mmel_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Convert the power spectrogram to decibels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\feature\\spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m     \"\"\"\n\u001b[0;32m   2143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2144\u001b[1;33m     S, n_fft = _spectrogram(\n\u001b[0m\u001b[0;32m   2145\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m         \u001b[0mS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[1;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m   2831\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Unable to compute spectrogram with n_fft={n_fft}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2832\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2833\u001b[1;33m             raise ParameterError(\n\u001b[0m\u001b[0;32m   2834\u001b[0m                 \u001b[1;34m\"Input signal must be provided to compute a spectrogram\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2835\u001b[0m             )\n",
      "\u001b[1;31mParameterError\u001b[0m: Input signal must be provided to compute a spectrogram"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def record_audio(duration=5, sr=22050):\n",
    "    \"\"\"\n",
    "    Records an audio clip for a given duration and sampling rate.\n",
    "    Returns the recorded audio signal as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Set up the PyAudio stream\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format = pyaudio.paInt16, channels = 1, rate = sr, input = True, frames_per_buffer = 1024)\n",
    "\n",
    "    # Record audio for the specified duration\n",
    "    frames = []\n",
    "    for i in range(int(sr / 1024 * duration)):\n",
    "        frames.append(stream.read(1024))\n",
    "\n",
    "    # Stop recording and close stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Convert the recorded audio to a NumPy array\n",
    "    audio = np.frombuffer(b''.join(frames), dtype = np.float64)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def extract_spectrogram(audio, sr, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Converts an audio signal to a Mel spectrogram.\n",
    "    Returns the spectrogram as a 2D NumPy array.\n",
    "    \"\"\"\n",
    "    # Compute the Mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(audio = audio, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # Convert the power spectrogram to decibels\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize the spectrogram between 0 and 1\n",
    "    norm_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "\n",
    "    return norm_mel_spec\n",
    "\n",
    "\n",
    "def predict_emotion(spectrogram):\n",
    "    \"\"\"\n",
    "    Predicts the emotion from a Mel spectrogram using a pre-trained model.\n",
    "    Returns a string indicating the predicted emotion.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model\n",
    "    model = load_model('Emotion_Voice_Detection_Model.h5')\n",
    "\n",
    "    # Add a channel dimension to the spectrogram for compatibility with the model\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "\n",
    "    # Make a prediction with the model\n",
    "    prediction = model.predict(np.array([spectrogram]))\n",
    "\n",
    "    # Get the predicted emotion\n",
    "    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    predicted_emotion = emotions[np.argmax(prediction)]\n",
    "\n",
    "    return predicted_emotion\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Record an audio clip\n",
    "    print('Press enter to start recording...')\n",
    "    input()\n",
    "    audio_1 = record_audio(\n",
    "\n",
    "    # Convert the audio to a spectrogram\n",
    "    spectrogram = extract_spectrogram(audio=audio_1, sr=22050)\n",
    "\n",
    "    # Predict the emotion from the spectrogram\n",
    "    predicted_emotion = predict_emotion(spectrogram)\n",
    "\n",
    "    print('Predicted emotion:', predicted_emotion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c7d660a-9623-4996-91b9-9be97c52ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 9.1834095e-41, 0.0000000e+00, ..., 6.6857749e-38,\n",
       "       3.4237611e-37, 6.6713013e-37], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8934e40-a4f1-47da-a413-57a79dcc3ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cb25fd9-9f07-4c13-a00c-b6562af7ba8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\vijay\\\\AppData\\\\Local\\\\Temp\\\\tmp9ac8bgip.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6812\\3539316321.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# Extract the spectrogram from the audio segment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mspectrogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_spectrogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Predict the stress level from the spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6812\\3539316321.py\u001b[0m in \u001b[0;36mextract_spectrogram\u001b[1;34m(audio)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Set up a temporary file to hold the audio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.wav'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp_audio_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_audio_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Load the audio into a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(self, out_f, format, codec, bitrate, parameters, tags, id3v2_version, cover)\u001b[0m\n\u001b[0;32m    865\u001b[0m                     'or call export(format=\"raw\") with no codec or parameters')\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[0mout_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fd_or_path_or_tempfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m         \u001b[0mout_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydub\\utils.py\u001b[0m in \u001b[0;36m_fd_or_path_or_tempfile\u001b[1;34m(fd, mode, tempfile)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mclose_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\vijay\\\\AppData\\\\Local\\\\Temp\\\\tmp9ac8bgip.wav'"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tempfile\n",
    "\n",
    "def extract_spectrogram(audio):\n",
    "    \"\"\"\n",
    "    Converts an audio signal to a Mel spectrogram.\n",
    "    Returns the spectrogram as a 2D NumPy array.\n",
    "    \"\"\"\n",
    "    # Set up a temporary file to hold the audio\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav') as temp_audio_file:\n",
    "        audio.export(temp_audio_file.name, format='wav')\n",
    "\n",
    "        # Load the audio into a numpy array\n",
    "        audio_array = AudioSegment.from_file(temp_audio_file.name)\n",
    "        audio_array = np.array(audio_array.get_array_of_samples())\n",
    "\n",
    "    # Compute the Mel spectrogram\n",
    "    sample_rate = audio.frame_rate\n",
    "    mel_spec = mfcc(audio_array, sample_rate)\n",
    "\n",
    "    # Convert the spectrogram to decibels\n",
    "    log_mel_spec = (10 * np.log10(mel_spec)).astype(np.float32)\n",
    "\n",
    "    # Normalize the spectrogram between 0 and 1\n",
    "    norm_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "\n",
    "    return norm_mel_spec\n",
    "\n",
    "\n",
    "def predict_stress(spectrogram):\n",
    "    \"\"\"\n",
    "    Predicts the stress level from a Mel spectrogram using a pre-trained model.\n",
    "    Returns a string indicating the predicted stress level.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model\n",
    "    model = load_model('Emotion_Voice_Detection_Model.h5')\n",
    "\n",
    "    # Add a channel dimension to the spectrogram for compatibility with the model\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "\n",
    "    # Make a prediction with the model\n",
    "    prediction = model.predict(np.array([spectrogram]))\n",
    "\n",
    "    # Get the predicted stress level\n",
    "    stress_levels = ['low', 'medium', 'high']\n",
    "    predicted_stress = stress_levels[np.argmax(prediction)]\n",
    "\n",
    "    return predicted_stress\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load an audio file\n",
    "    audio_file = 'file_example_WAV_1MG.wav'\n",
    "    audio = AudioSegment.from_file(audio_file, format='wav')\n",
    "\n",
    "    # Split the audio into segments of silence\n",
    "    chunks = split_on_silence(audio, min_silence_len=500, silence_thresh=-50)\n",
    "\n",
    "    # Iterate over each segment and predict the stress level\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extract the spectrogram from the audio segment\n",
    "        spectrogram = extract_spectrogram(chunk)\n",
    "\n",
    "        # Predict the stress level from the spectrogram\n",
    "        predicted_stress = predict_stress(spectrogram)\n",
    "\n",
    "        print(f'Segment {i+1}: Predicted stress level is {predicted_stress}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02d02c08-5a6d-4df6-9efe-cf53fa14c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio signal: [-5.7928264e-07  7.7905133e-07 -1.0112999e-06 ...  8.7503594e-04\n",
      "  9.6312381e-04  1.3055827e-03]\n",
      "Sample rate: 22050\n"
     ]
    }
   ],
   "source": [
    "print('Audio signal:', audio)\n",
    "print('Sample rate:', sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89d3df79-6e75-462a-afc4-0188d12b0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a24cc8ca-b0e9-43f5-8f68-dd462c612be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load('audio.wav', sr=44100)\n",
    "tonnetz = librosa.feature.tonnetz(y=data, sr=sr)\n",
    "rms = librosa.feature.rms(y=data)\n",
    "mfcc = librosa.feature.mfcc(y=data, sr=sr)\n",
    "features = np.vstack((tonnetz, rms, mfcc)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d21567a-72fe-4d66-bab3-961471d9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.load_model('Emotion_Voice_Detection_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11924713-1a0b-4154-a834-88ad33b9bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data, sr = librosa.load('audio.wav', sr=44100)\n",
    "new_tonnetz = librosa.feature.tonnetz(y=new_data, sr=sr)\n",
    "new_rms = librosa.feature.rms(y=new_data)\n",
    "new_mfcc = librosa.feature.mfcc(y=new_data, sr=sr)\n",
    "new_features = np.vstack((new_tonnetz, new_rms, new_mfcc)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdc82e-81c8-4138-95ee-05dbc50a93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Scale the new features using the same scaler used for training\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "new_scaled_features = scaler.transform(new_features)\n",
    "\n",
    "# Predict the stress level for the new data\n",
    "new_stress_level = model.predict(new_scaled_features)\n",
    "\n",
    "print('Predicted stress level:', new_stress_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0625f896-e207-4872-b4b5-16d0c1386033",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 40, 1), found shape=(None, 5062, 27)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6812\\349921241.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Make predictions on the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Get the predicted emotion label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\vijay\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 40, 1), found shape=(None, 5062, 27)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = tf.keras.models.load_model('Emotion_Voice_Detection_Model.h5')\n",
    "\n",
    "# Load the audio file\n",
    "new_data, sr = librosa.load('audio.wav', sr=44100)\n",
    "\n",
    "# Extract features from the audio file\n",
    "new_tonnetz = librosa.feature.tonnetz(y=new_data, sr=sr)\n",
    "new_rms = librosa.feature.rms(y=new_data)\n",
    "new_mfcc = librosa.feature.mfcc(y=new_data, sr=sr)\n",
    "new_features = np.vstack((new_tonnetz, new_rms, new_mfcc)).T\n",
    "\n",
    "# Normalize the features\n",
    "\n",
    "new_features = (new_features - mean) / std\n",
    "\n",
    "# Reshape the features to match the input shape of the model\n",
    "new_features = np.expand_dims(new_features, axis=0)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_features)\n",
    "\n",
    "# Get the predicted emotion label\n",
    "labels = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad']\n",
    "predicted_label = labels[np.argmax(predictions)]\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a43cc3b-0575-41d7-a47e-0f2bfe52d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2351bb8e-5c6b-4aeb-a026-db47977d0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "214fcec8-8ae3-41d4-96b7-3801e16a8b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.96010757e-03, -2.14120007e-03,  7.61812925e-03, ...,\n",
       "          1.36522565e-03,  3.10432818e-03,  1.76108000e-03],\n",
       "        [-2.10836290e-02,  1.18089759e-02,  1.34631172e-02, ...,\n",
       "         -2.23122883e+00, -6.20985746e+00, -5.20122910e+00],\n",
       "        [-5.14188400e-02,  5.06867759e-02,  3.54062021e-03, ...,\n",
       "          1.98323834e+00, -8.81096554e+00, -7.14596701e+00],\n",
       "        ...,\n",
       "        [ 4.65991381e-02, -1.69077473e-02, -1.46428555e-01, ...,\n",
       "          1.78913498e+00,  4.16780090e+00,  3.10919523e+00],\n",
       "        [ 5.50630935e-02, -1.19720721e-02, -1.10003833e-01, ...,\n",
       "         -1.08747435e+00, -3.69550109e+00, -4.78567696e+00],\n",
       "        [ 5.30827025e-02, -8.62660451e-03, -9.30352807e-02, ...,\n",
       "         -1.63402128e+00, -3.51941943e+00, -4.25034046e+00]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d96d4-6bb4-4e5e-acc6-ea974c146b99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
